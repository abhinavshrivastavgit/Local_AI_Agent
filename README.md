# ğŸš€ Local AI RAG System â€” Private Document Chatbot

A fully local **Retrieval-Augmented Generation (RAG)** system that allows you to chat with your own data using local LLMs â€” without sending any information to external APIs.

This project demonstrates how modern AI systems used in companies actually work internally.

---

# ğŸ§  What This Project Does

This system allows users to:

- Load a dataset or documents
- Convert them into embeddings
- Store them inside a vector database
- Retrieve relevant context
- Generate answers using a **local LLM**

Everything runs **completely offline**.

No OpenAI API.  
No external services.  
100% local AI.

---

# ğŸ— System Architecture

User Question  
â†“  
Retriever (Vector Search - ChromaDB)  
â†“  
Relevant Context Retrieved  
â†“  
Prompt Template  
â†“  
Local LLM (Ollama - Llama Model)  
â†“  
Generated Answer

This is the same architecture used in many production AI systems.

---

# ğŸ”¥ Why This Project Is Important

Most developers only use API-based AI tools.

But real AI engineers build systems that:

- Protect sensitive data
- Work offline
- Scale with internal datasets
- Use retrieval-based intelligence

This project demonstrates that workflow.

---

# âš™ï¸ Tech Stack

**LLM Runtime**  
Ollama

**LLM Model**  
Llama 3.2

**Embeddings Model**  
nomic-embed-text

**Vector Database**  
ChromaDB

**AI Framework**  
LangChain

**Programming Language**  
Python

**Dataset**  
Restaurant Reviews CSV

---

# ğŸ“‚ Project Structure

```
Local_AI_AGENT
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ realistic_restaurant_reviews.csv
â”‚
â”œâ”€â”€ vector.py
â”‚   Handles:
â”‚   - Data loading
â”‚   - Document conversion
â”‚   - Embeddings
â”‚   - Vector database creation
â”‚
â”œâ”€â”€ main.py
â”‚   Runs:
â”‚   - Chat interface
â”‚   - Retrieval system
â”‚   - LLM responses
â”‚
â”œâ”€â”€ chroma_db/
â”‚   Vector database storage
â”‚
â”œâ”€â”€ requirements.txt
â”‚   Project dependencies
â”‚
â””â”€â”€ README.md
```

---

# ğŸ§© Core Components

### 1. Data Loader
Reads dataset from CSV file using Pandas.

### 2. Document Processor
Converts dataset rows into LangChain documents.

### 3. Embedding Generator
Creates semantic embeddings using:

```
nomic-embed-text
```

### 4. Vector Database
Stores embeddings using:

```
ChromaDB
```

### 5. Retriever
Searches for the most relevant context based on the user question.

### 6. Local LLM
Generates final answers using:

```
Llama 3.2
```

---

# ğŸ›  Installation

## Step 1 â€” Install Ollama

Download from:

https://ollama.com/

---

## Step 2 â€” Pull Required Models

Run this in terminal:

```
ollama pull llama3.2
ollama pull nomic-embed-text
```

---

## Step 3 â€” Install Python Dependencies

```
pip install -r requirements.txt
```

If requirements file is missing:

```
pip install chromadb
pip install langchain
pip install langchain-ollama
pip install langchain-community
pip install pandas
pip install pypdf
```

---

# â–¶ï¸ Running the Project

Start Ollama:

```
ollama serve
```

Run the chatbot:

```
python main.py
```

---

# ğŸ’¬ Example Questions

You can ask:

- Which restaurant has the best reviews?
- What do customers complain about the most?
- Summarize customer feedback.
- Which place has the best pizza?

---

# ğŸ§ª Learning Goals Behind This Project

This project helped me understand:

- How RAG systems actually work
- How embeddings power semantic search
- How vector databases store knowledge
- How local LLMs can replace cloud APIs
- How to structure real AI projects

---

# ğŸš€ Future Improvements

Planned upgrades:

- PDF document support
- Multi-dataset ingestion
- Web UI interface
- Conversation memory
- n8n automation integration
- API endpoint for deployment
- Docker containerization

---

# ğŸ‘¨â€ğŸ’» Author

Abhinav Shrivastav

Building projects in:
- AI
- Data Engineering
- Automation
- Local LLM Systems

---

# ğŸŒŸ Project Vision

The goal is to move from:

Using AI tools  
to  
Building AI systems.

---

If you found this interesting, feel free to connect or follow the journey.
